{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51d60854",
   "metadata": {},
   "source": [
    "# VGG 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1776d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c2a093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5541b1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing other required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras import Sequential\n",
    "from keras.applications import VGG19 #For Transfer Learning\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Flatten,Dense,BatchNormalization,Activation,Dropout\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61b37f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train,y_train,test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72e3a4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((35000, 32, 32, 3), (35000, 1))\n",
      "((15000, 32, 32, 3), (15000, 1))\n",
      "((10000, 32, 32, 3), (10000, 1))\n"
     ]
    }
   ],
   "source": [
    "#Dimension of the dataset\n",
    "print((x_train.shape,y_train.shape))\n",
    "print((x_val.shape,y_val.shape))\n",
    "print((x_test.shape,y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2288d51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#One Hot Encoding\n",
    "y_train=to_categorical(y_train)\n",
    "y_val=to_categorical(y_val)\n",
    "y_test=to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acfe5948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((35000, 32, 32, 3), (35000, 10))\n",
      "((15000, 32, 32, 3), (15000, 10))\n",
      "((10000, 32, 32, 3), (10000, 10))\n"
     ]
    }
   ],
   "source": [
    "#Verifying the dimension after one hot encoding\n",
    "print((x_train.shape,y_train.shape))\n",
    "print((x_val.shape,y_val.shape))\n",
    "print((x_test.shape,y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a86eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we will perform the image data augmentation. This is the technique that is used to expand the size of a training dataset by creating modified versions of images in the dataset. First, we will define individual instances of ImageDataGenerator for augmentation and then we will fit them with each of the training, test and validation datasets. \n",
    "\n",
    "#Image Data Augmentation\n",
    "train_generator = ImageDataGenerator(rotation_range=2, horizontal_flip=True, zoom_range=.1)\n",
    "\n",
    "val_generator = ImageDataGenerator(rotation_range=2, horizontal_flip=True, zoom_range=.1)\n",
    "\n",
    "test_generator = ImageDataGenerator(rotation_range=2, horizontal_flip= True, zoom_range=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3328c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the augmentation defined above to the data\n",
    "train_generator.fit(x_train)\n",
    "val_generator.fit(x_val)\n",
    "test_generator.fit(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eba5024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning Rate Annealer\n",
    "lrr= ReduceLROnPlateau(monitor='val_acc', factor=.01, patience=3, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a6d6028",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the VGG Convolutional Neural Net\n",
    "base_model = VGG19(include_top = False, weights = 'imagenet', input_shape = (32,32,3), classes = y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbf4c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the final layers to the above base models where the actual classification is done in the dense layers\n",
    "model= Sequential()\n",
    "model.add(base_model) \n",
    "model.add(Flatten()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1c2c346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg19 (Functional)          (None, 1, 1, 512)         20024384  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afbccf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the Dense layers along with activation and batch normalization\n",
    "model.add(Dense(1024,activation=('relu'),input_dim=512))\n",
    "model.add(Dense(512,activation=('relu'))) \n",
    "model.add(Dense(256,activation=('relu'))) \n",
    "model.add(Dropout(.3)),Dense(128,activation=('relu'))\n",
    "#model.add(Dropout(.2))\n",
    "model.add(Dense(10,activation=('softmax'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c161b9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg19 (Functional)          (None, 1, 1, 512)         20024384  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              525312    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,208,394\n",
      "Trainable params: 21,208,394\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Checking the final model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "155cf8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the hyperparameters\n",
    "batch_size= 32\n",
    "epochs = 50\n",
    "learn_rate=.001\n",
    "sgd = SGD(lr=learn_rate,momentum=.9,nesterov=False)\n",
    "adam = Adam(learning_rate=learn_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be84a64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - ETA: 0s - loss: 2.3029 - accuracy: 0.0960WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "350/350 [==============================] - 924s 3s/step - loss: 2.3029 - accuracy: 0.0960 - val_loss: 2.3027 - val_accuracy: 0.1006 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "import time\n",
    "tic = time.time()\n",
    "model.fit(train_generator.flow(\n",
    "                    x_train, \n",
    "                    y_train, \n",
    "                    batch_size= 32),\n",
    "                    epochs = 1, \n",
    "                    steps_per_epoch = x_train.shape[0]//batch_size,\n",
    "                    validation_data = val_generator.flow(x_val, y_val, \n",
    "                    batch_size = 32), \n",
    "                    validation_steps = 250, \n",
    "                    callbacks=[lrr], \n",
    "                    verbose = 1)\n",
    "toc = time.time()\n",
    "# training VGG19 deep convolutional neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dda3d8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 924.2557187080383 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f'time {toc-tic} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba92a928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x19d082ab9d0>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the training and validation loss and accuracy\n",
    "f,ax=plt.subplots(2,1) \n",
    "\n",
    "#Loss\n",
    "ax[0].plot(model.history.history['loss'],color='b',label='Training Loss')\n",
    "ax[0].plot(model.history.history['val_loss'],color='r',label='Validation Loss')\n",
    "\n",
    "#Accuracy\n",
    "ax[1].plot(model.history.history['accuracy'],color='b',label='Training  Accuracy')\n",
    "ax[1].plot(model.history.history['val_accuracy'],color='r',label='Validation Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2234f8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 89s 284ms/step\n"
     ]
    }
   ],
   "source": [
    "#Making prediction\n",
    "y_pred = np.argmax(model.predict(x_test), axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "94713eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.argmax(y_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "075f0f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 132s 282ms/step\n"
     ]
    }
   ],
   "source": [
    "val_pred = np.argmax(model.predict(x_val), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "184170ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function for confusion matrix plot\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "                #Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "#print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7,7))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "\n",
    "    #Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e891ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70274568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9a4302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce25350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
